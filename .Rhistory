`7/8 - 7/14`,
`7/15 - 7/21`,
`7/22 - 7/28`,
`7/29-8/4`,
`8/5-8/11`,
`8/12-8/18`,
`8/19-8/25`,
`8/26-9/1`,
`9/2-9/8`) # remove this column has all 0's and not in Pyj-- need to revisit this!
View(P_yj)
View(commercial_effort_yi)
# these are the proportions of run present at each week
P_yj <-  read.csv("data/Kusko_Reconstruction/P_yj_Data_V1.csv") %>%
janitor::row_to_names(row_number = 1) %>%
slice(1:36) %>%
dplyr::mutate(year = 1976:2011) %>%
gather(1:15, key = "date", value = "proportion") %>%
dplyr::mutate(proportion = as.numeric(gsub(",","",proportion))) %>%
replace(is.na(.), 0)  %>%
#dates are out of order, need to arrange them in order
spread(date, proportion)  %>%
dplyr::select(#`5/27 - 6/2`, # effort data doesnt have these first two columns and they are 0s
#`6/3 - 6/9`,
`6/10 - 6/16`,
`6/17 - 6/23`,
`6/24 - 6/30`,
`7/1 - 7/7`,
`7/8 - 7/14`,
`7/15 - 7/21`,
`7/22 - 7/28`,
`7/29-8/4`,
`8/5-8/11`,
`8/12-8/18`,
`8/19-8/25`,
`8/26-9/1`,
`9/2-9/8`) # remove this column has all 0's and not in Pyj-- need to revisit this!
#total returns to be compared with N_hat...
N_y = rowSums(escapement) + rowSums(catch_y) # Eq. 6
#Eq. 4 -- not sure if this is actually Nyhat because nothing was estimated?? unlcear in paper.
#total returns per week/year is a fraction of the fish that are within the bethel area (Pyj)
N_yj_hat = as.matrix(N_y*P_yj)#[,2:14])
# want error for each week and year -- Log normal Error
error_yj = matrix( ncol = ncol(P_yj)-1, nrow = nrow(P_yj))
for (i in 1:ncol(P_yj)-1 ) {
error_yj[,i] = rlnorm(n= 36)#, meanlog = 0, sdlog= sd(commercial_catch_yi[,i]))
}
#calculate catchability, q = catch/effort (assumping catch = mortality??)
mat_commercial_catch_yi<- as.matrix(commercial_catch_yi)
mat_commercial_effort_yi<-as.matrix(commercial_effort_yi)
View(mat_commercial_effort_yi)
View(mat_commercial_catch_yi)
#select 2:14 to remove the year column for division
q=mat_commercial_catch_yi[,2:14]/mat_commercial_effort_yi[,2:14]
#select 2:14 to remove the year column for division
q=mat_commercial_catch_yi[,2:14]/mat_commercial_effort_yi
q[is.nan(q)] <- 0
#B_yj is bethel efort week/year
B_yj<-mat_commercial_effort_yi
###### Baranov Catch for each week/year, yj
C_yj_hat = matrix( ncol = ncol(P_yj), nrow = nrow(P_yj))
for (i in 1:ncol(P_yj) ) {
C_yj_hat[,i] = N_yj_hat[,i]*(1-exp(-q[,i]*B_yj[,i]))*exp(error_yj[,i])
}
i=1
N_yj_hat[,i]
q[,i]
B_yj[,i]
error_yj[,i]
# want error for each week and year -- Log normal Error
error_yj = matrix( ncol = ncol(P_yj), nrow = nrow(P_yj))
for (i in 1:ncol(P_yj) ) {
error_yj[,i] = rlnorm(n= 36)#, meanlog = 0, sdlog= sd(commercial_catch_yi[,i]))
}
#calculate catchability, q = catch/effort (assumping catch = mortality??)
mat_commercial_catch_yi<- as.matrix(commercial_catch_yi)
mat_commercial_effort_yi<-as.matrix(commercial_effort_yi)
#select 2:14 to remove the year column for division
q=mat_commercial_catch_yi[,2:14]/mat_commercial_effort_yi
q[is.nan(q)] <- 0
#B_yj is bethel efort week/year
B_yj<-mat_commercial_effort_yi
###### Baranov Catch for each week/year, yj
C_yj_hat = matrix( ncol = ncol(P_yj), nrow = nrow(P_yj))
for (i in 1:ncol(P_yj) ) {
C_yj_hat[,i] = N_yj_hat[,i]*(1-exp(-q[,i]*B_yj[,i]))*exp(error_yj[,i])
}
View(C_yj_hat)
C_yj_hat[is.nan(C_yj_hat)] <- 0
error_yj
mean(error_yj)
# want error for each week and year -- Log normal Error
error_yj = matrix( ncol = ncol(P_yj), nrow = nrow(P_yj))
for (i in 1:ncol(P_yj) ) {
error_yj[,i] = rlnorm(n= 36, meanlog = 0, sdlog= 1) #sd(commercial_catch_yi[,i]))
}
mean(error_yj)
t<-rlnorm(n=1000, meanlog = 0, sdlog= 1)
mean(t)
t<-rlnorm(n=1000, meanlog = 0, sdlog= 1)
log(mean(t))
t<-rlnorm(n=100000, meanlog = 0, sdlog= 1)
log(mean(t))
# want error for each week and year -- Log normal Error
error_yj = matrix( ncol = ncol(P_yj), nrow = nrow(P_yj))
for (i in 1:ncol(P_yj) ) {
error_yj[,i] = rlnorm(n= 36, meanlog = 0, sdlog= 1) #sd(commercial_catch_yi[,i]))
}
#calculate catchability, q = catch/effort (assumping catch = mortality??)
mat_commercial_catch_yi<- as.matrix(commercial_catch_yi)
mat_commercial_effort_yi<-as.matrix(commercial_effort_yi)
#select 2:14 to remove the year column for division
q=mat_commercial_catch_yi[,2:14]/mat_commercial_effort_yi
q[is.nan(q)] <- 0
#B_yj is bethel efort week/year
B_yj<-mat_commercial_effort_yi
###### Baranov Catch for each week/year, yj
C_yj_hat = matrix( ncol = ncol(P_yj), nrow = nrow(P_yj))
for (i in 1:ncol(P_yj) ) {
C_yj_hat[,i] = N_yj_hat[,i]*(1-exp(-q[,i]*B_yj[,i]))*exp(error_yj[,i])
}
C_yj_hat[is.nan(C_yj_hat)] <- 0
names(N_yj_hat)
names(P_yj)
names(C_yj_hat) <- names(P_yj)
View(C_yj_hat)
names(C_yj_hat)[1:13] <- names(P_yj)
View(C_yj_hat)
names(data.frame(C_yj_hat)) <- names(P_yj)
data.frame(C_yj_hat)
names(data.frame(C_yj_hat)) <- names(P_yj)
colnames(C_yj_hat) <- names(P_yj)
View(C_yj_hat)
View(C_yj_hat)
# Exercise 1: Loading Required Packages ========================================
library(tidyverse)
library(ggthemes)
library(dplyr)
library(wisp)
library(lubridate)
# Install FSAdata package for some example depletion datasets
# install.packages("FSAdata")
library(FSAdata)
# First let's load an example dataset for exploring our depletion estimator, from
# Polovina, J.J. 1985. "A variable catchability version of the Leslie model
#    with application to an intensive fishing experiment on a multispecies stock."
data(Pathfinder)
str(Pathfinder)
head(Pathfinder)
# First lets visualize our data
list.pf <- Pathfinder %>% pivot_longer(-c(date,effort),
names_to="species", values_to="catch")
str(list.pf)
#calculate CPUE
cpue = mat_commercial_catch_yi[,2:14]/mat_commercial_effort_yi
View(cpue)
View(mat_commercial_catch_yi)
View(mat_commercial_effort_yi)
# Next, lets calculate catch-per-unit-effort
list.pf <- list.pf %>% mutate("cpue"=catch/effort)
head(list.pf)
# Now we can plot CPUE across time
ggplot(data=list.pf, aes(x=date, y=cpue, color=species)) +
geom_line() +
geom_point() +
facet_wrap(~species, ncol=1, scales="free_y")
k = cumsum(mat_commercial_catch_yi[,2:14])
k
View(mat_commercial_catch_yi)
k=apply(mat_commercial_catch_yi[,2:14], 2, cumsum)
View(k)
View(k)
View(mat_commercial_catch_yi)
k_lag = k-mat_commercial_catch_yi[,2:14]
View(k_lag)
View(k)
View(k_lag)
list.pf <- list.pf %>% group_by(species) %>% mutate("K_lag"=K-catch)
# And the cumulative catch (K) across sampling events
list.pf <- list.pf %>% group_by(species) %>% mutate("K"=cumsum(catch))
head(list.pf)
#  Let's double-check this worked correctly, yes cpue is now aligned with the
#  K from the prior time step
list.pf %>% filter(species=="Pzonatus")
list.pf %>% filter(species=="Pauricilla")
list.pf %>% filter(species=="Ecarbunculus")
# Now lets plot cpue_t ~ K_t-1
ggplot(data=list.pf, aes(x=K_lag, y=cpue, color=species)) +
geom_line() +
geom_point() +
facet_wrap(~species, ncol=1, scales="free_y")
# Next, lets calculate catch-per-unit-effort
list.pf <- list.pf %>% mutate("cpue"=catch/effort)
# Now we can plot CPUE across time
ggplot(data=list.pf, aes(x=date, y=cpue, color=species)) +
geom_line() +
geom_point() +
facet_wrap(~species, ncol=1, scales="free_y")
# And the cumulative catch (K) across sampling events
list.pf <- list.pf %>% group_by(species) %>% mutate("K"=cumsum(catch))
head(list.pf)
# Let's double-check this worked correctly
list.pf %>% filter(species=="Pzonatus")
list.pf %>% filter(species=="Pauricilla")
list.pf %>% filter(species=="Ecarbunculus")
# So we need to create a lagged K variable
?lag
list.pf <- list.pf %>% group_by(species) %>% mutate("K_lag"=K-catch)
head(list.pf)
#  Let's double-check this worked correctly, yes cpue is now aligned with the
#  K from the prior time step
list.pf %>% filter(species=="Pzonatus")
list.pf %>% filter(species=="Pauricilla")
list.pf %>% filter(species=="Ecarbunculus")
# Now lets plot cpue_t ~ K_t-1
ggplot(data=list.pf, aes(x=K_lag, y=cpue, color=species)) +
geom_line() +
geom_point() +
facet_wrap(~species, ncol=1, scales="free_y")
cpue = t(mat_commercial_catch_yi)
View(cpue)
t(mat_commercial_catch_yi[,2:14])
cpue = t(mat_commercial_catch_yi[,2:14])
View(cpue)
#obs_effort, B_yj eq 5 -- this needs to be tidy-ed
catch_effort <-read.csv("data/Kusko_Reconstruction/Bethel_Effort.csv", header = FALSE)
View(catch_effort)
#everything needs to be rotated to have year across top and and month down the columns
#calculate CPUE, catch/effort
cpue = t(mat_commercial_catch_yi[,2:14])/t(mat_commercial_effort_yi)
k = apply(t(mat_commercial_catch_yi[,2:14]), 2, cumsum)
k_lag = k-t(mat_commercial_catch_yi[,2:14])
View(k_lag)
cpue = t(mat_commercial_catch_yi[,2:14])
View(cpue)
catch_df <- data.frame(mat_commercial_catch_yi)
View(catch_df)
catch_df <- data.frame(t(mat_commercial_catch_yi))
catch_df <- data.frame(t(mat_commercial_catch_yi)) %>%
row_to_names(row_number = 1)
catch_df <- data.frame(t(mat_commercial_catch_yi)) %>%
janitor::row_to_names(row_number = 1)
# Process:
# Calculate CPUE, and Kt (Cumulative CPUE)
# Fit regression model
# Extract intercept
# Divide intercept by the slope (%)
catch_df <- data.frame(t(mat_commercial_catch_yi)) %>%
janitor::row_to_names(row_number = 1)
# Process:
# Calculate CPUE, and Kt (Cumulative CPUE)
# Fit regression model
# Extract intercept
# Divide intercept by the slope (%)
catch_df <- data.frame(t(mat_commercial_catch_yi)) %>%
janitor::row_to_names(row_number = 1) %>%
gather( 1:36, key = "year", value = "catch")
catch_df <- data.frame(t(mat_commercial_catch_yi)) %>%
janitor::row_to_names(row_number = 1)
# Divide intercept by the slope (%)
catch_df <- data.frame(t(mat_commercial_catch_yi)) %>%
janitor::row_to_names(row_number = 1) %>%
tibble::rownames_to_column()
# Process:
# Calculate CPUE, and Kt (Cumulative CPUE)
# Fit regression model
# Extract intercept
# Divide intercept by the slope (%)
catch_df <- data.frame(t(mat_commercial_catch_yi)) %>%
janitor::row_to_names(row_number = 1) %>%
tibble::rownames_to_column(var = "week") %>%
gather( 2:37, key = "year", value = "catch")
effort_df <- data.frame(t(mat_commercial_effort_yi)) %>%
janitor::row_to_names(row_number = 1) %>%
tibble::rownames_to_column(var = "week")
effort_df <- data.frame(t(mat_commercial_effort_yi)) %>%
janitor::row_to_names(row_number = 1)
effort_df <- data.frame(t(mat_commercial_effort_yi))
View(effort_df)
commercial_effort_yi <- commercial_effort_df %>%
slice(-2) %>%
janitor::row_to_names(row_number = 1) %>%
dplyr::mutate(year = 1976:2011)
View(commercial_effort_yi)
commercial_effort_yi <- commercial_effort_df %>%
slice(-2) %>%
janitor::row_to_names(row_number = 1) %>%
dplyr::mutate(year = 1976:2011)  %>%
gather(1:13, key = "date", value = "effort") %>%
dplyr::mutate(effort=as.numeric(gsub(",","",effort))) %>%
spread(date, effort)
commercial_effort_yi <- commercial_effort_df %>%
slice(-2) %>%
janitor::row_to_names(row_number = 1) %>%
dplyr::mutate(year = 1976:2011)  %>%
gather(1:13, key = "date", value = "effort") %>%
dplyr::mutate(effort=as.numeric(gsub(",","",effort))) %>%
spread(date, effort) %>%
dplyr::select(year, #`5/27 - 6/2`,
#`6/3 - 6/9`,
`6/10 - 6/16`,
`6/17 - 6/23`,
`6/24 - 6/30`,
`7/1 - 7/7`,
`7/8 - 7/14`,
`7/15 - 7/21`,
`7/22 - 7/28`,
`7/29-8/4`,
`8/5-8/11`,
`8/12-8/18`,
`8/19-8/25`,
`8/26-9/1`,
`9/2-9/8`) # remove this column has all 0's and not in Pyj-- need to revisit this!
commercial_effort_yi <- commercial_effort_df %>%
slice(-2) %>%
janitor::row_to_names(row_number = 1) %>%
dplyr::mutate(year = 1976:2011)  %>%
gather(1:13, key = "date", value = "effort") %>%
dplyr::mutate(effort=as.numeric(gsub(",","",effort))) %>%
spread(date, effort) %>%
dplyr::select(year, #`5/27 - 6/2`,
#`6/3 - 6/9`,
`6/10 - 6/16`,
`6/17 - 6/23`,
`6/24 - 6/30`,
`7/1 - 7/7`,
`7/8 - 7/14`,
`7/15 - 7/21`,
`7/22 - 7/28`,
`7/29-8/4`,
`8/5-8/11`,
`8/12-8/18`,
`8/19-8/25`,
`8/26-9/1`,
`9/2-9/8`) # remove this column has all 0's and not in Pyj-- need to revisit this!
effort_df <- data.frame(t(mat_commercial_effort_yi)) %>%
janitor::row_to_names(row_number = 1) %>%
tibble::rownames_to_column(var = "week") %>%
gather( 2:37, key = "year", value = "effort")
effort_df <- data.frame(t(mat_commercial_effort_yi)) %>%
janitor::row_to_names(row_number = 1) %>%
tibble::rownames_to_column(var = "week")
effort_df <- data.frame(t(mat_commercial_effort_yi))
View(effort_df)
commercial_effort_yi <- commercial_effort_df %>%
slice(-2) %>%
janitor::row_to_names(row_number = 1) %>%
dplyr::mutate(year = 1976:2011)  %>%
gather(1:13, key = "date", value = "effort") %>%
dplyr::mutate(effort=as.numeric(gsub(",","",effort))) %>%
spread(date, effort) %>%
dplyr::select(year, #`5/27 - 6/2`,
#`6/3 - 6/9`,
`6/10 - 6/16`,
`6/17 - 6/23`,
`6/24 - 6/30`,
`7/1 - 7/7`,
`7/8 - 7/14`,
`7/15 - 7/21`,
`7/22 - 7/28`,
`7/29-8/4`,
`8/5-8/11`,
`8/12-8/18`,
`8/19-8/25`,
`8/26-9/1`,
`9/2-9/8`) # remove this column has all 0's and not in Pyj-- need to revisit this!
commercial_effort_yi <- commercial_effort_df %>%
slice(-2) %>%
janitor::row_to_names(row_number = 1) %>%
dplyr::mutate(year = 1976:2011)  %>%
gather(1:13, key = "date", value = "effort")
commercial_catch_yi <- commercial_catch_df %>%
slice(-2) %>%
janitor::row_to_names(row_number = 1) %>%
dplyr::mutate(year = 1976:2011)  %>%
gather(1:13, key = "date", value = "catch") #%>%
commercial_effort_df<- catch_effort[ , col_odd == 0]
commercial_effort_yi <- commercial_effort_df %>%
slice(-2) %>%
janitor::row_to_names(row_number = 1) %>%
dplyr::mutate(year = 1976:2011)  %>%
gather(1:13, key = "date", value = "effort")
View(commercial_catch_yi)
# Process:
# Calculate CPUE, and Kt (Cumulative CPUE)
# Fit regression model
# Extract intercept
# Divide intercept by the slope (%)
cpue_df = left_join(commercial_catch_yi, commercial_effort_yi)
View(cpue_df)
View(cpue_df)
# Process:
# Calculate CPUE, and Kt (Cumulative CPUE)
# Fit regression model
# Extract intercept
# Divide intercept by the slope (%)
cpue_df = left_join(commercial_catch_yi, commercial_effort_yi) %>%
dplyr::mutate(cpue = catch/effort) %>%
group_by(year) %>%
dplyr::mutate(
k=cumsum(catch),
k_lag = (k-catch))
# Process:
# Calculate CPUE, and Kt (Cumulative CPUE)
# Fit regression model
# Extract intercept
# Divide intercept by the slope (%)
cpue_df = left_join(commercial_catch_yi, commercial_effort_yi) %>%
dplyr::mutate(catch=as.numeric(catch),
effort = as.numeric(effort),
cpue = catch/effort) %>%
group_by(year) %>%
dplyr::mutate(
k=cumsum(catch),
k_lag = (k-catch))
View(cpue_df)
# Process:
# Calculate CPUE, and Kt (Cumulative CPUE)
# Fit regression model
# Extract intercept
# Divide intercept by the slope (%)
cpue_df = left_join(commercial_catch_yi, commercial_effort_yi) %>%
dplyr::mutate(catch=as.numeric(catch),
effort = as.numeric(effort),
cpue = catch/effort,
cpue = case_when(is.nan(cpue)~ 0,
TRUE ~ cpue)) %>%
group_by(year) %>%
dplyr::mutate(
k=cumsum(catch),
k_lag = (k-catch))
lm.84 <- lm(cpue ~ k_lag, data=cpue_df %>% filter(year == 1984))
# We will recall from lecture that our slope is our estimate of catchability (q)
q.pz <- abs(coef(lm.84)[2])
q.pz
coef(lm.84)
lm.84
summary(lm.84)
# We will recall from lecture that our slope is our estimate of catchability (q)
q.pz <- abs(coef(lm.84)[2])
q.pz
# And our intercept is equal to q*N0, where N0 is our initial population size
int.pz <- coef(lm.84)[1]
int.pz
N0.pz <- int.pz/q.pz
N0.pz
years<-unique(cpue_df$year)
source("~/Documents/GitHub/AYK_prey/scripts/Kusko_ML_Run_Reconstruction .R", echo=TRUE)
years<-unique(cpue_df$year)
length(years)
q_df <- data.frame(year = years,
q= rep(NA,times = length(years)),
N0 = rep(NA,times = length(years)))
View(q_df)
q.pz
years<-unique(cpue_df$year)
i=4
years[i]
reg_df <- cpue_df %>%
filter(year == years[i])
lm <- lm(cpue ~ k_lag, data=reg_df)
# We will recall from lecture that our slope is our estimate of catchability (q)
q.pz <- abs(coef(lm)[2])
# And our intercept is equal to q*N0, where N0 is our initial population size
int.pz <- coef(lm)[1]
# We can calculate our estimate of initial abundance at the beginning of the
#   depletion experiment by dividing our intercept (q*N0) by q
N0.pz <- int.pz/q.pz
q_df[i,2]<-q.pz
q_df[i,3]<-N0.pz
View(q_df)
years<-unique(cpue_df$year)
q_df <- data.frame(year = years,
q= rep(NA,times = length(years)),
N0 = rep(NA,times = length(years)))
for (i in 1:length(years)) {
reg_df <- cpue_df %>%
filter(year == years[i])
lm <- lm(cpue ~ k_lag, data=reg_df)
# We will recall from lecture that our slope is our estimate of catchability (q)
q.pz <- abs(coef(lm)[2])
# And our intercept is equal to q*N0, where N0 is our initial population size
int.pz <- coef(lm)[1]
# We can calculate our estimate of initial abundance at the beginning of the
#   depletion experiment by dividing our intercept (q*N0) by q
N0.pz <- int.pz/q.pz
q_df[i,2]<-q.pz
q_df[i,3]<-N0.pz
}
View(q_df)
q[,i]
View(q_df)
is.na(q_df) <- 0
View(q_df)
is.na(q_df$q) <- 0
View(q_df)
q_df[is.na(q_df)] <- 0
View(q_df)
ncol(P_yj)
View(P_yj)
q_vec<-t(q_df$q)
View(q_vec)
N_yj_hat[,i]
View(N_yj_hat)
q_vec<- q_df$q
for (i in 1:ncol(P_yj) ) {
C_yj_hat[,i] = N_yj_hat[,i]*(1-exp(-q_vec*B_yj[,i]))*exp(error_yj[,i])
}
C_yj_hat[is.nan(C_yj_hat)] <- 0
colnames(C_yj_hat) <- names(P_yj)
View(C_yj_hat)
N0.pz
View(q_df)
